# python3 -m local_py.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/gigaspeech_dev/wav.scp,speech,kaldi_ark --valid_data_path_and_name_and_type dump/raw/gigaspeech_dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --resume true --init_param --ignore_init_mismatch false --fold_length 80000 --fold_length 150 --output_dir exp/asr_train_asr_conformer6_n_fft512_hop_length256_rnnt_raw_en_bpe5000 --config conf/tuning/train_asr_conformer6_n_fft512_hop_length256_rnnt.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/gigaspeech_train_m/wav.scp,speech,kaldi_ark --train_data_path_and_name_and_type dump/raw/gigaspeech_train_m/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --ngpu 4 --multiprocessing_distributed True 
# Started at Thu Nov 25 12:03:41 CST 2021
#
/DB/rhome/chenyuyang/miniconda3/envs/espnet/bin/python3 /DB/rhome/chenyuyang/projects/asr1/local_py/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/gigaspeech_dev/wav.scp,speech,kaldi_ark --valid_data_path_and_name_and_type dump/raw/gigaspeech_dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --resume true --init_param --ignore_init_mismatch false --fold_length 80000 --fold_length 150 --output_dir exp/asr_train_asr_conformer6_n_fft512_hop_length256_rnnt_raw_en_bpe5000 --config conf/tuning/train_asr_conformer6_n_fft512_hop_length256_rnnt.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/gigaspeech_train_m/wav.scp,speech,kaldi_ark --train_data_path_and_name_and_type dump/raw/gigaspeech_train_m/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --ngpu 4 --multiprocessing_distributed True
[DB20:0/4] 2021-11-25 12:05:09,945 (distributed_c10d:217) INFO: Added key: store_based_barrier_key:1 to store for rank: 0
[DB20:0/4] 2021-11-25 12:05:09,945 (distributed_c10d:251) INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
[DB20:0/4] 2021-11-25 12:05:10,242 (asr:362) INFO: Vocabulary size: 5000
[DB20:0/4] 2021-11-25 12:05:10,457 (conformer_encoder:131) WARNING: Using legacy_rel_pos and it will be deprecated in the future.
[DB20:0/4] 2021-11-25 12:05:10,662 (conformer_encoder:231) WARNING: Using legacy_rel_selfattn and it will be deprecated in the future.
[DB20:0/4] 2021-11-25 12:05:28,218 (abs_task:1151) INFO: pytorch.version=1.10.0, cuda.available=True, cudnn.version=7605, cudnn.benchmark=False, cudnn.deterministic=True
[DB20:0/4] 2021-11-25 12:05:28,228 (abs_task:1152) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=256, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): ConformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=9728, out_features=512, bias=True)
        (1): LegacyRelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=512, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
  (rnnt_decoder): RNNT(
    (dec): RNNDecoder(
      (embed): Embedding(5000, 256, padding_idx=0)
      (dropout_embed): Dropout(p=0.0, inplace=False)
      (decoder): ModuleList(
        (0): LSTM(256, 256, batch_first=True)
        (1): LSTM(256, 256, batch_first=True)
      )
      (dropout_dec): Dropout(p=0.2, inplace=False)
    )
    (joint_network): JointNetwork(
      (lin_enc): Linear(in_features=512, out_features=512, bias=True)
      (lin_dec): Linear(in_features=256, out_features=512, bias=False)
      (lin_out): Linear(in_features=512, out_features=5000, bias=True)
      (joint_activation): Tanh()
    )
    (transducer_loss): RNNTLoss()
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 91.09 M
    Number of trainable parameters: 91.09 M (100.0%)
    Size: 364.35 MB
    Type: torch.float32
[DB20:0/4] 2021-11-25 12:05:28,259 (abs_task:1155) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0015
    lr: 6.000000000000001e-08
    weight_decay: 0
)
[DB20:0/4] 2021-11-25 12:05:28,259 (abs_task:1156) INFO: Scheduler: WarmupLR(warmup_steps=25000)
[DB20:0/4] 2021-11-25 12:05:28,306 (abs_task:1165) INFO: Saving the configuration in exp/asr_train_asr_conformer6_n_fft512_hop_length256_rnnt_raw_en_bpe5000/config.yaml
[DB20:0/4] 2021-11-25 12:05:42,582 (abs_task:1515) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/gigaspeech_train_m/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/gigaspeech_train_m/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f28d8224c40>)
[DB20:0/4] 2021-11-25 12:05:42,582 (abs_task:1516) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=61081, batch_bins=2000000, sort_in_batch=descending, sort_batch=descending)
[DB20:0/4] 2021-11-25 12:05:42,595 (abs_task:1517) INFO: [train] mini-batch sizes summary: N-batch=61081, mean=14.9, min=4, max=115
[DB20:0/4] 2021-11-25 12:05:42,917 (abs_task:1515) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/gigaspeech_dev/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/gigaspeech_dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f28d8224d90>)
[DB20:0/4] 2021-11-25 12:05:42,917 (abs_task:1516) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=686, batch_bins=2000000, sort_in_batch=descending, sort_batch=descending)
[DB20:0/4] 2021-11-25 12:05:42,917 (abs_task:1517) INFO: [valid] mini-batch sizes summary: N-batch=686, mean=8.3, min=4, max=60
[DB20:0/4] 2021-11-25 12:05:42,957 (abs_task:1515) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/gigaspeech_dev/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/gigaspeech_dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f28670e8be0>)
[DB20:0/4] 2021-11-25 12:05:42,957 (abs_task:1516) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=5715, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[DB20:0/4] 2021-11-25 12:05:42,957 (abs_task:1517) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
DB20:23480:23480 [0] NCCL INFO Bootstrap : Using enp129s0f0:192.168.28.130<0>
DB20:23480:23480 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
DB20:23480:23480 [0] NCCL INFO NET/IB : No device found.
DB20:23480:23480 [0] NCCL INFO NET/Socket : Using [0]enp129s0f0:192.168.28.130<0> [1]enp129s0f1:172.16.0.130<0>
DB20:23480:23480 [0] NCCL INFO Using network Socket
NCCL version 2.10.3+cuda10.2
DB20:23481:23481 [1] NCCL INFO Bootstrap : Using enp129s0f0:192.168.28.130<0>
DB20:23481:23481 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
DB20:23481:23481 [1] NCCL INFO NET/IB : No device found.
DB20:23481:23481 [1] NCCL INFO NET/Socket : Using [0]enp129s0f0:192.168.28.130<0> [1]enp129s0f1:172.16.0.130<0>
DB20:23481:23481 [1] NCCL INFO Using network Socket
DB20:23483:23483 [3] NCCL INFO Bootstrap : Using enp129s0f0:192.168.28.130<0>
DB20:23483:23483 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
DB20:23483:23483 [3] NCCL INFO NET/IB : No device found.
DB20:23483:23483 [3] NCCL INFO NET/Socket : Using [0]enp129s0f0:192.168.28.130<0> [1]enp129s0f1:172.16.0.130<0>
DB20:23483:23483 [3] NCCL INFO Using network Socket
DB20:23482:23482 [2] NCCL INFO Bootstrap : Using enp129s0f0:192.168.28.130<0>
DB20:23482:23482 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
DB20:23482:23482 [2] NCCL INFO NET/IB : No device found.
DB20:23482:23482 [2] NCCL INFO NET/Socket : Using [0]enp129s0f0:192.168.28.130<0> [1]enp129s0f1:172.16.0.130<0>
DB20:23482:23482 [2] NCCL INFO Using network Socket
DB20:23480:23820 [0] NCCL INFO Channel 00/02 :    0   1   2   3
DB20:23481:23821 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
DB20:23480:23820 [0] NCCL INFO Channel 01/02 :    0   1   2   3
DB20:23480:23820 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
DB20:23481:23821 [1] NCCL INFO Setting affinity for GPU 8 to 3fffff
DB20:23480:23820 [0] NCCL INFO Setting affinity for GPU 7 to 3fffff
DB20:23482:23825 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
DB20:23482:23825 [2] NCCL INFO Setting affinity for GPU 9 to 3fffff
DB20:23483:23824 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
DB20:23483:23824 [3] NCCL INFO Setting affinity for GPU 3 to 3fffff
DB20:23481:23821 [1] NCCL INFO Channel 00 : 1[e000] -> 2[f000] via P2P/IPC
DB20:23481:23821 [1] NCCL INFO Channel 01 : 1[e000] -> 2[f000] via P2P/IPC
DB20:23482:23825 [2] NCCL INFO Channel 00 : 2[f000] -> 3[7000] via direct shared memory
DB20:23482:23825 [2] NCCL INFO Channel 01 : 2[f000] -> 3[7000] via direct shared memory
DB20:23483:23824 [3] NCCL INFO Channel 00 : 3[7000] -> 0[d000] via direct shared memory
DB20:23483:23824 [3] NCCL INFO Channel 01 : 3[7000] -> 0[d000] via direct shared memory
DB20:23480:23820 [0] NCCL INFO Channel 00 : 0[d000] -> 1[e000] via P2P/IPC
DB20:23480:23820 [0] NCCL INFO Channel 01 : 0[d000] -> 1[e000] via P2P/IPC
DB20:23482:23825 [2] NCCL INFO Connected all rings
DB20:23483:23824 [3] NCCL INFO Connected all rings
DB20:23483:23824 [3] NCCL INFO Channel 00 : 3[7000] -> 2[f000] via direct shared memory
DB20:23481:23821 [1] NCCL INFO Connected all rings
DB20:23480:23820 [0] NCCL INFO Connected all rings
DB20:23483:23824 [3] NCCL INFO Channel 01 : 3[7000] -> 2[f000] via direct shared memory
DB20:23481:23821 [1] NCCL INFO Channel 00 : 1[e000] -> 0[d000] via P2P/IPC
DB20:23481:23821 [1] NCCL INFO Channel 01 : 1[e000] -> 0[d000] via P2P/IPC
DB20:23480:23820 [0] NCCL INFO Connected all trees
DB20:23480:23820 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
DB20:23480:23820 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
DB20:23482:23825 [2] NCCL INFO Channel 00 : 2[f000] -> 1[e000] via P2P/IPC
DB20:23482:23825 [2] NCCL INFO Channel 01 : 2[f000] -> 1[e000] via P2P/IPC
DB20:23483:23824 [3] NCCL INFO Connected all trees
DB20:23483:23824 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
DB20:23483:23824 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
DB20:23481:23821 [1] NCCL INFO Connected all trees
DB20:23481:23821 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
DB20:23481:23821 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
DB20:23482:23825 [2] NCCL INFO Connected all trees
DB20:23482:23825 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
DB20:23482:23825 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
DB20:23482:23825 [2] NCCL INFO comm 0x7f4f6c000fb0 rank 2 nranks 4 cudaDev 2 busId f000 - Init COMPLETE
DB20:23481:23821 [1] NCCL INFO comm 0x7f5bdc000fb0 rank 1 nranks 4 cudaDev 1 busId e000 - Init COMPLETE
DB20:23480:23820 [0] NCCL INFO comm 0x7f2850000fb0 rank 0 nranks 4 cudaDev 0 busId d000 - Init COMPLETE
DB20:23483:23824 [3] NCCL INFO comm 0x7f86dc000fb0 rank 3 nranks 4 cudaDev 3 busId 7000 - Init COMPLETE
DB20:23480:23480 [0] NCCL INFO Launch mode Parallel
[DB20:0/4] 2021-11-25 12:05:52,744 (trainer:274) INFO: 1/20epoch started
/DB/rhome/chenyuyang/projects/asr1/espnet2/layers/stft.py:166: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  olens = (ilens - self.win_length) // self.hop_length + 1
[DB20:0/4] 2021-11-25 12:07:16,954 (distributed:874) INFO: Reducer buckets have been rebuilt in this iteration.
[DB20:0/4] 2021-11-25 14:05:22,182 (trainer:668) INFO: 1epoch:train:1-3054batch: iter_time=0.004, forward_time=1.900, loss=115.329, loss_ctc=129.438, loss_rnnt=50.610, cer_rnnt=0.962, wer_rnnt=1.000, backward_time=0.089, optim_step_time=0.061, optim0_lr0=2.298e-05, train_time=9.394
[DB20:0/4] 2021-11-25 14:06:23,073 (ctc:74) WARNING: 1/8 samples got nan grad. These were ignored for CTC loss.
[DB20:0/4] 2021-11-25 14:38:41,754 (trainer:668) INFO: 1epoch:train:3055-6108batch: iter_time=4.959e-04, forward_time=0.423, loss=85.107, loss_ctc=95.569, loss_rnnt=37.322, cer_rnnt=0.961, wer_rnnt=1.000, backward_time=0.090, optim_step_time=0.061, optim0_lr0=6.879e-05, train_time=2.619
[DB20:0/4] 2021-11-25 15:14:03,810 (trainer:668) INFO: 1epoch:train:6109-9162batch: iter_time=0.003, forward_time=0.454, loss=83.950, loss_ctc=94.236, loss_rnnt=36.832, cer_rnnt=0.970, wer_rnnt=1.000, backward_time=0.090, optim_step_time=0.061, optim0_lr0=1.146e-04, train_time=2.779
[DB20:0/4] 2021-11-25 15:49:38,649 (trainer:668) INFO: 1epoch:train:9163-12216batch: iter_time=1.991e-04, forward_time=0.464, loss=81.551, loss_ctc=91.702, loss_rnnt=35.700, cer_rnnt=0.964, wer_rnnt=1.000, backward_time=0.088, optim_step_time=0.060, optim0_lr0=1.604e-04, train_time=2.796
